\documentclass[12pt]{article}

\RequirePackage{amsmath}
\RequirePackage{amsthm}
\RequirePackage{amssymb}
\RequirePackage[mathscr]{eucal}
\RequirePackage{mathtools}
\RequirePackage{etoolbox}
\usepackage[red]{zhoucx-notation}

\geometry{letterpaper, top = 1in, bottom = 1in, left = 1.5in, right = 1in}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\declaretheorem[numbered=no]{definition}

\theoremstyle{theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\renewcommand{\qedsymbol}{\hfill\rule{2mm}{2mm}}

%\pagestyle{fancy}
%\fancyhf{}
%\setlength{\headheight}{15pt}
%\rhead{\textsf{June 13, 2022}}
%\lhead{\textsf{Chenxi Zhou}}
%\renewcommand{\headrulewidth}{1pt}
%\cfoot{\thepage}

\title{ \setstretch{1.0} \textbf{ \Large Abstract}}
\author{}
\date{}

\allowdisplaybreaks
\setstretch{2}

\begin{document}

\thispagestyle{plain}
\maketitle

% ------------------------------------------------------------------

This dissertation is concerned with the nonparametric density estimation problem in infinite-dimensional kernel exponential families, which amounts to minimizing convex loss functionals in an infinite-dimensional reproducing kernel Hilbert space. The loss functionals we focus on are the negative log-likelihood (NLL) loss functional and the score matching (SM) loss functional. 

We propose a new density estimator called the early stopping SM density estimator, which is obtained by applying the gradient descent algorithm to minimizing the SM loss functional and terminating the algorithm early. We both theoretically and empirically investigate various aspects of this density estimator. We also compare this early stopping SM density estimator with the penalized SM density estimator that has been studied in the literature and address their similarities and differences. 

In addition, we propose an adaptive algorithm to compute the penalized maximum likelihood (ML) density estimator that is obtained by minimizing the penalized NLL loss functional. We empirically compare the regularized SM and ML density estimators and find out that when there is a small amount of regularization, the regularized SM density estimators contain a bump or become a spike at the isolated observation, but the regularized ML density estimator does not. Moreover, if we remove the isolated observation, the bump or the spike in regularized SM density estimators disappears when the regularization is small. We attempt to explain why this happens. 

The presence of a bump or a spike at the isolated observation in SM density estimators motivate us to study the sensitivity of density estimators to the presence of an additional observation. We extend the definition of the influence function and allow its input to be function-valued statistical functionals. We study various properties of this extended influence function in finite-dimensional and kernel exponential families, and empirically demonstrate that regularized SM density estimators in a kernel exponential family is more sensitive to the presence of an additional observation than the regularized ML density estimator when the amount of regularization is small. 

\end{document}
